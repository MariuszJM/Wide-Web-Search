{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca363d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_llm = \"llama3.2:latest\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd290dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "\n",
    "def summarize_text_map_reduce(llm, docs, token_max: int):\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=token_max, chunk_overlap=0)\n",
    "\n",
    "    map_prompt = ChatPromptTemplate.from_messages([(\"human\", \"You are an expert content summarizer. Combine your understanding of the following into a detailed nested bullet point summary:\\n\\n{context}\")])\n",
    "    reduce_template = \"\"\"\n",
    "    The following is a set of summaries:\n",
    "    {docs}\n",
    "    Combine all of your understanding into a single, detailed nested bullet point summary with overview at the beggining.\n",
    "    \"\"\"\n",
    "    reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
    "    \n",
    "    map_chain = map_prompt | llm | StrOutputParser()\n",
    "    reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
    "\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    summaries = [map_chain.invoke(doc.page_content) for doc in split_docs]\n",
    "\n",
    "    def length_function(docs):\n",
    "        return sum(llm.get_num_tokens(doc) for doc in docs)\n",
    "\n",
    "    def chunk_summaries(summaries, max_tokens):\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_chunk_tokens = 0\n",
    "        \n",
    "        for summary in summaries:\n",
    "            summary_tokens = llm.get_num_tokens(summary)\n",
    "            \n",
    "            if current_chunk_tokens + summary_tokens <= max_tokens:\n",
    "                current_chunk.append(summary)\n",
    "                current_chunk_tokens += summary_tokens\n",
    "            else:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = [summary]\n",
    "                current_chunk_tokens = summary_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    while length_function(summaries) > token_max:\n",
    "        chunks = chunk_summaries(summaries, token_max)\n",
    "        new_summaries = []\n",
    "        for chunk in chunks:\n",
    "            chunk_text = \"\\n\\n\".join(chunk)\n",
    "            new_summaries.append(reduce_chain.invoke(chunk_text))\n",
    "        summaries = new_summaries\n",
    "\n",
    "    final_summary = reduce_chain.invoke(summaries)\n",
    "    \n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ef4902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "def create_index_retriver(docs_list):    \n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1000, chunk_overlap=200\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    # Add to vectorDB\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\", device=\"nvidia\"),\n",
    "    )\n",
    "\n",
    "    # Create retriever\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k':1})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8883aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "def is_chunk_relevant(chunk, question, llm_json_mode):\n",
    "    # Doc grader instructions\n",
    "    doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "    # Grader prompt\n",
    "    doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "    This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "    Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "   \n",
    "    doc_grader_prompt_formatted = doc_grader_prompt.format(document=chunk, question=question)\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=doc_grader_instructions)]\n",
    "        + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "    )\n",
    "    grade = json.loads(result.content)[\"binary_score\"]\n",
    "    return grade.lower() == \"yes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bdf9ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(question, relevant_chunks, llm):\n",
    "    rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "    Here is the context to use to answer the question:\n",
    "\n",
    "    {context} \n",
    "\n",
    "    Think carefully about the above context. \n",
    "\n",
    "    Now, review the user question:\n",
    "\n",
    "    {question}\n",
    "\n",
    "    Provide an answer to this questions using only the above context. \n",
    "\n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    context = format_docs(relevant_chunks)\n",
    "    rag_prompt_formatted = rag_prompt.format(context=context, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return generation.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80055fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hallucination_grader(answer, relevant_chunks, llm_json_mode):\n",
    "    \n",
    "    # Hallucination grader instructions\n",
    "    hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "    You are a teacher grading a quiz. \n",
    "\n",
    "    You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "    Here is the grade criteria to follow:\n",
    "\n",
    "    (1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "    (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "    Score:\n",
    "\n",
    "    A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "    A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "    Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "    Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "    # Grader prompt\n",
    "    hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "    Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)    \n",
    "    \n",
    "\n",
    "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
    "        documents=format_docs(relevant_chunks), generation=answer\n",
    "    )\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=hallucination_grader_instructions)]\n",
    "        + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
    "    )\n",
    "    grade = json.loads(result.content)[\"binary_score\"]\n",
    "    return grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a707828",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_QUERIES = [\n",
    "    \"High-level overview of intelligent user interfaces and their impact on modern UI/UX design\",\n",
    "    \"Tools and frameworks for building intelligent user interfaces: A 2024 guide\"\n",
    "]\n",
    "\n",
    "SPECIFIC_QUESTIONS = [\n",
    "    \"What are the latest trends in intelligent user interfaces, and how are they shaping user experience?\",\n",
    "    \"What are the best practices for ensuring accessibility and inclusivity in AI-powered user interfaces?\",\n",
    "]\n",
    "\n",
    "TIME_HORIZON = 185  \n",
    "\n",
    "# Max Outputs\n",
    "MAX_OUTPUTS = 1\n",
    "\n",
    "PLATFORM = 'google'\n",
    "SOURCES_PER_QUERY = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864da8e9",
   "metadata": {},
   "source": [
    "# google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a676a3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, recent research has focused on developing context-aware concept evaluation approaches (Wang et al., 2021) and graph-based requirement elicitation frameworks (Wang et al., 2019; Wang et al., 2021) for smart product-service systems. These trends aim to create more dynamic and adaptive interfaces that consider user emotions and contexts, as suggested by Wattearachchi et al.'s framework (2020). By incorporating these intelligent features, user experience is expected to be enhanced through more personalized and responsive interactions.\n",
      "Hallucination check passed\n",
      "Based on the provided context, ensuring accessibility and inclusivity in AI-powered user interfaces can be achieved through a rule-based approach, as introduced by Stephanidis et al. (1998). This methodology involves dynamically adjusting the UI to individual user needs, particularly in terms of accessibility, using predefined rules to govern how the interface should adapt under specific conditions. By leveraging contextual data and authoring tools, AUIs can provide personalized interface adjustments that cater to diverse user needs, promoting enhanced usability and inclusivity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 10088, which is longer than the specified 7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination check passed\n",
      "The latest trend in intelligent user interfaces is the integration of Artificial Intelligence (AI) with User Experience (UX) design, enabling enhanced personalisation and automation of design workflows. AI-powered tools can generate multiple design variations, speed up the design process, and ensure consistency across different elements of the interface. This synergy is transforming digital landscapes, reshaping how users interact with the world around them through more intuitive and responsive interfaces.\n",
      "Hallucination check passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1348 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant information item\n",
      "The latest trend in intelligent user interfaces is the shift towards gesture-based interactions, which are becoming increasingly popular due to their natural and intuitive nature. Gesture-based UIs are revolutionizing the way users interact with applications, offering a more direct and engaging experience that can increase user satisfaction and loyalty. By recognizing and adapting to users' gestures and movements, modern devices and applications are delivering innovative solutions while outlining new challenges for developers to overcome.\n",
      "Hallucination check passed\n",
      "To ensure accessibility and inclusivity in AI-powered user interfaces, it is crucial to consider users with limited hand mobility and provide alternative methods for interaction, such as voice commands or haptic feedback. Developers should also establish clear and intuitive gestures that are immediately apparent, but if not, offer visual prompts and guidance to direct the user. Additionally, thorough testing across various platforms and screens is necessary to confirm usability and avoid accessibility issues.\n",
      "Hallucination check passed\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_community import GoogleSearchAPIWrapper\n",
    "import os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "if not os.getenv(\"GOOGLE_API_KEY\") or not os.getenv(\"GOOGLE_CSE_ID\"):\n",
    "    raise EnvironmentError(\"Missing GOOGLE_API_KEY or GOOGLE_CSE_ID environment variables.\")\n",
    "\n",
    "search = GoogleSearchAPIWrapper()\n",
    "unique_urls = set()\n",
    "\n",
    "for search_query in SEARCH_QUERIES:\n",
    "    results = search.results(search_query, SOURCES_PER_QUERY, search_params={'dateRestrict': f'd{TIME_HORIZON}', 'gl': 'EN'})\n",
    "    urls = [item['link'] for item in results]\n",
    "    unique_urls.update(urls)\n",
    "\n",
    "source_items = {}\n",
    "for url in unique_urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    doc = loader.load()\n",
    "    title = doc[0].metadata.get('title', url)\n",
    "    source_items[title] = {'url': url, 'page_content': doc[0].page_content, 'qa':{}}\n",
    "    retriver = create_index_retriver(doc)\n",
    "    for question in SPECIFIC_QUESTIONS:\n",
    "        semantic_search_chunks = retriver.invoke(question)\n",
    "        relevant_chunks = []\n",
    "        for chunk in semantic_search_chunks:\n",
    "            if is_chunk_relevant(chunk, question, llm_json_mode):  \n",
    "                relevant_chunks.append(chunk)\n",
    "        if len(relevant_chunks)==0:\n",
    "            break\n",
    "        answer = generate(question, relevant_chunks, llm)\n",
    "        print(answer)\n",
    "        is_answer_make_sense = hallucination_grader(answer, relevant_chunks, llm_json_mode)\n",
    "        if is_answer_make_sense == \"yes\":\n",
    "            print(\"Hallucination check passed\")\n",
    "            source_items[title]['qa'][question] = answer \n",
    "        else:\n",
    "            print(\"Hallucination check failed\")\n",
    "            break\n",
    "    question_relevance_score = len(source_items[title]['qa'])\n",
    "    if question_relevance_score == 0:\n",
    "        source_items.pop(title)\n",
    "        print(\"No relevant information item\")\n",
    "    else:\n",
    "        summary = summarize_text_map_reduce(llm, doc, 7500)\n",
    "        source_items[title]['summary'] = summary\n",
    "        source_items[title]['question_relevance_score'] = question_relevance_score\n",
    "    \n",
    "ranked_data_items = dict(sorted(source_items.items(), key=lambda x: x[1]['question_relevance_score'], reverse=True))\n",
    "\n",
    "MAX_OUTPUTS = min(MAX_OUTPUTS, len(ranked_data_items)) \n",
    "\n",
    "top_data = {}\n",
    "less_relevant_data = {}\n",
    "for i, (title, meta_data) in enumerate(ranked_data_items.items()):\n",
    "    if i < MAX_OUTPUTS:\n",
    "        top_data[title] = meta_data\n",
    "    else:\n",
    "        less_relevant_data[title] = meta_data\n",
    "\n",
    "def remove_relevance_score(data):\n",
    "    for item in data.values():\n",
    "        if 'question_relevance_score' in item:\n",
    "            del item['question_relevance_score']\n",
    "    return data\n",
    "\n",
    "top_data = remove_relevance_score(top_data)\n",
    "less_relevant_data = remove_relevance_score(less_relevant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8108c",
   "metadata": {},
   "source": [
    "# Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75d17594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m['https://www.youtube.com/watch?v=nS1UrJnncWc&pp=ygVaSGlnaC1sZXZlbCBvdmVydmlldyBvZiBpbnRlbGxpZ2VudCB1c2VyIGludGVyZmFjZXMgYW5kIHRoZWlyIGltcGFjdCBvbiBtb2Rlcm4gVUkvVVggZGVzaWdu', 'https://www.youtube.com/watch?v=XZf5A0wcruE&pp=ygVaSGlnaC1sZXZlbCBvdmVydmlldyBvZiBpbnRlbGxpZ2VudCB1c2VyIGludGVyZmFjZXMgYW5kIHRoZWlyIGltcGFjdCBvbiBtb2Rlcm4gVUkvVVggZGVzaWdu']\u001b[0m\u001b[32;1m\u001b[1;3m['https://www.youtube.com/watch?v=MOyl58VF2ak&pp=ygVLVG9vbHMgYW5kIGZyYW1ld29ya3MgZm9yIGJ1aWxkaW5nIGludGVsbGlnZW50IHVzZXIgaW50ZXJmYWNlczogQSAyMDI0IGd1aWRl', 'https://www.youtube.com/watch?v=B6tn6ojJ2wg&pp=ygVLVG9vbHMgYW5kIGZyYW1ld29ya3MgZm9yIGJ1aWxkaW5nIGludGVsbGlnZW50IHVzZXIgaW50ZXJmYWNlczogQSAyMDI0IGd1aWRl']\u001b[0mNo relevant information item\n",
      "No relevant information item\n",
      "No relevant information item\n",
      "No relevant information item\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, YoutubeLoader\n",
    "import os\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "import ast\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "if not os.getenv(\"GOOGLE_API_KEY\") or not os.getenv(\"GOOGLE_CSE_ID\"):\n",
    "    raise EnvironmentError(\"Missing GOOGLE_API_KEY or GOOGLE_CSE_ID environment variables.\")\n",
    "\n",
    "tool = YouTubeSearchTool()\n",
    "unique_urls = set()\n",
    "for search_query in SEARCH_QUERIES:\n",
    "    urls_str = tool.run(search_query, 2 * SOURCES_PER_QUERY)\n",
    "    urls = set(ast.literal_eval(urls_str))\n",
    "    unique_urls = unique_urls | urls\n",
    "\n",
    "unique_urls = list(unique_urls)\n",
    "\n",
    "source_items = {}\n",
    "for url in unique_urls:\n",
    "    loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)\n",
    "    doc = loader.load()\n",
    "    title = doc[0].metadata.get('title', url)\n",
    "    source_items[title] = {'url': url, 'page_content': doc[0].page_content, 'qa':{}}\n",
    "    \n",
    "    published_date_str = doc[0].metadata.get('publish_date', None)\n",
    "    published_date = datetime.strptime(published_date_str, '%Y-%m-%d %H:%M:%S')\n",
    "    if datetime.now() - published_date <= timedelta(days=TIME_HORIZON):\n",
    "        source_items.pop(title)\n",
    "        break\n",
    "    \n",
    "    retriver = create_index_retriver(doc)\n",
    "    for question in SPECIFIC_QUESTIONS:\n",
    "        semantic_search_chunks = retriver.invoke(question)\n",
    "        relevant_chunks = []\n",
    "        for chunk in semantic_search_chunks:\n",
    "            if is_chunk_relevant(chunk, question, llm_json_mode):  \n",
    "                relevant_chunks.append(chunk)\n",
    "        if len(relevant_chunks)==0:\n",
    "            break\n",
    "        answer = generate(question, relevant_chunks, llm)\n",
    "        print(answer)\n",
    "        is_answer_make_sense = hallucination_grader(answer, relevant_chunks, llm_json_mode)\n",
    "        if is_answer_make_sense == \"yes\":\n",
    "            print(\"Hallucination check passed\")\n",
    "            source_items[title]['qa'][question] = answer \n",
    "        else:\n",
    "            print(\"Hallucination check failed\")\n",
    "            break\n",
    "    question_relevance_score = len(source_items[title]['qa'])\n",
    "    if question_relevance_score == 0:\n",
    "        source_items.pop(title)\n",
    "        print(\"No relevant information item\")\n",
    "    else:\n",
    "        summary = summarize_text_map_reduce(llm, doc, 7500)\n",
    "        source_items[title]['summary'] = summary\n",
    "        source_items[title]['question_relevance_score'] = question_relevance_score\n",
    "    \n",
    "ranked_data_items = dict(sorted(source_items.items(), key=lambda x: x[1]['question_relevance_score'], reverse=True))\n",
    "\n",
    "MAX_OUTPUTS = min(MAX_OUTPUTS, len(ranked_data_items)) \n",
    "\n",
    "top_data = {}\n",
    "less_relevant_data = {}\n",
    "for i, (title, meta_data) in enumerate(ranked_data_items.items()):\n",
    "    if i < MAX_OUTPUTS:\n",
    "        top_data[title] = meta_data\n",
    "    else:\n",
    "        less_relevant_data[title] = meta_data\n",
    "\n",
    "def remove_relevance_score(data):\n",
    "    for item in data.values():\n",
    "        if 'question_relevance_score' in item:\n",
    "            del item['question_relevance_score']\n",
    "    return data\n",
    "\n",
    "top_data = remove_relevance_score(top_data)\n",
    "less_relevant_data = remove_relevance_score(less_relevant_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
