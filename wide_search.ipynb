{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7ca363d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "### LLM\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "local_llm = \"llama3.2:latest\"\n",
        "llm = ChatOllama(model=local_llm, temperature=0)\n",
        "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fd290dc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from typing import List\n",
        "\n",
        "def summarize_text_map_reduce(llm, docs, token_max: int):\n",
        "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=token_max, chunk_overlap=0)\n",
        "\n",
        "    map_prompt = ChatPromptTemplate.from_messages([(\"human\", \"You are an expert content summarizer. Combine your understanding of the following into a detailed nested bullet point summary:\\n\\n{context}\")])\n",
        "    reduce_template = \"\"\"\n",
        "    The following is a set of summaries:\n",
        "    {docs}\n",
        "    Combine all of your understanding into a single, detailed nested bullet point summary with overview at the beggining.\n",
        "    \"\"\"\n",
        "    reduce_prompt = ChatPromptTemplate([(\"human\", reduce_template)])\n",
        "    \n",
        "    map_chain = map_prompt | llm | StrOutputParser()\n",
        "    reduce_chain = reduce_prompt | llm | StrOutputParser()\n",
        "\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "    summaries = [map_chain.invoke(doc.page_content) for doc in split_docs]\n",
        "\n",
        "    def length_function(docs):\n",
        "        return sum(llm.get_num_tokens(doc) for doc in docs)\n",
        "\n",
        "    def chunk_summaries(summaries, max_tokens):\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_chunk_tokens = 0\n",
        "        \n",
        "        for summary in summaries:\n",
        "            summary_tokens = llm.get_num_tokens(summary)\n",
        "            \n",
        "            if current_chunk_tokens + summary_tokens <= max_tokens:\n",
        "                current_chunk.append(summary)\n",
        "                current_chunk_tokens += summary_tokens\n",
        "            else:\n",
        "                chunks.append(current_chunk)\n",
        "                current_chunk = [summary]\n",
        "                current_chunk_tokens = summary_tokens\n",
        "        \n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk)\n",
        "        \n",
        "        return chunks\n",
        "\n",
        "    while length_function(summaries) > token_max:\n",
        "        chunks = chunk_summaries(summaries, token_max)\n",
        "        new_summaries = []\n",
        "        for chunk in chunks:\n",
        "            chunk_text = \"\\n\\n\".join(chunk)\n",
        "            new_summaries.append(reduce_chain.invoke(chunk_text))\n",
        "        summaries = new_summaries\n",
        "\n",
        "    final_summary = reduce_chain.invoke(summaries)\n",
        "    \n",
        "    return final_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9ef4902c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import SKLearnVectorStore\n",
        "from langchain_nomic.embeddings import NomicEmbeddings\n",
        "\n",
        "def create_index_retriver(docs_list):    \n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        chunk_size=1000, chunk_overlap=200\n",
        "    )\n",
        "    doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "    # Add to vectorDB\n",
        "    vectorstore = SKLearnVectorStore.from_documents(\n",
        "        documents=doc_splits,\n",
        "        embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\", device=\"nvidia\"),\n",
        "    )\n",
        "\n",
        "    # Create retriever\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={'k':1})\n",
        "    return retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8883aaff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "\n",
        "def is_chunk_relevant(chunk, question, llm_json_mode):\n",
        "    # Doc grader instructions\n",
        "    doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
        "\n",
        "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
        "\n",
        "    # Grader prompt\n",
        "    doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
        "\n",
        "    This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
        "\n",
        "    Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
        "   \n",
        "    doc_grader_prompt_formatted = doc_grader_prompt.format(document=chunk, question=question)\n",
        "    result = llm_json_mode.invoke(\n",
        "        [SystemMessage(content=doc_grader_instructions)]\n",
        "        + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
        "    )\n",
        "    grade = json.loads(result.content)[\"binary_score\"]\n",
        "    return grade.lower() == \"yes\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9bdf9ed6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(question, relevant_chunks, llm):\n",
        "    rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
        "\n",
        "    Here is the context to use to answer the question:\n",
        "\n",
        "    {context} \n",
        "\n",
        "    Think carefully about the above context. \n",
        "\n",
        "    Now, review the user question:\n",
        "\n",
        "    {question}\n",
        "\n",
        "    Provide an answer to this questions using only the above context. \n",
        "\n",
        "    Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "    Answer:\"\"\"\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    context = format_docs(relevant_chunks)\n",
        "    rag_prompt_formatted = rag_prompt.format(context=context, question=question)\n",
        "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
        "    return generation.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "80055fb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def hallucination_grader(answer, relevant_chunks, llm_json_mode):\n",
        "    \n",
        "    # Hallucination grader instructions\n",
        "    hallucination_grader_instructions = \"\"\"\n",
        "\n",
        "    You are a teacher grading a quiz. \n",
        "\n",
        "    You will be given FACTS and a STUDENT ANSWER. \n",
        "\n",
        "    Here is the grade criteria to follow:\n",
        "\n",
        "    (1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
        "\n",
        "    (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
        "\n",
        "    Score:\n",
        "\n",
        "    A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
        "\n",
        "    A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
        "\n",
        "    Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
        "\n",
        "    Avoid simply stating the correct answer at the outset.\"\"\"\n",
        "\n",
        "    # Grader prompt\n",
        "    hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
        "\n",
        "    Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
        "\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)    \n",
        "    \n",
        "\n",
        "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
        "        documents=format_docs(relevant_chunks), generation=answer\n",
        "    )\n",
        "    result = llm_json_mode.invoke(\n",
        "        [SystemMessage(content=hallucination_grader_instructions)]\n",
        "        + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
        "    )\n",
        "    grade = json.loads(result.content)[\"binary_score\"]\n",
        "    return grade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a707828",
      "metadata": {},
      "outputs": [],
      "source": [
        "SEARCH_QUERIES = [\n",
        "    \"High-level overview of intelligent user interfaces and their impact on modern UI/UX design\",\n",
        "    \"Tools and frameworks for building intelligent user interfaces: A 2024 guide\"\n",
        "]\n",
        "\n",
        "CONTENT_QUESTIONS  = [\n",
        "    \"What are the latest trends in intelligent user interfaces, and how are they shaping user experience?\",\n",
        "    \"What are the best practices for ensuring accessibility and inclusivity in AI-powered user interfaces?\",\n",
        "]\n",
        "\n",
        "TIME_HORIZON = 185  \n",
        "\n",
        "# Max Outputs\n",
        "MAX_OUTPUTS = 1\n",
        "\n",
        "PLATFORM = 'google'\n",
        "MAX_SOURCES_PER_SEARCH_QUERY = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "864da8e9",
      "metadata": {},
      "source": [
        "# google\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a676a3f5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
            "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, recent research has focused on developing context-aware concept evaluation approaches (Wang et al., 2021) and graph-based requirement elicitation frameworks (Wang et al., 2019; Wang et al., 2021) for smart product-service systems. These trends aim to create more dynamic and adaptive interfaces that consider user emotions and contexts, as suggested by Wattearachchi et al.'s framework (2020). By incorporating these intelligent features, user experience is expected to be enhanced through more personalized and responsive interactions.\n",
            "Hallucination check passed\n",
            "Based on the provided context, ensuring accessibility and inclusivity in AI-powered user interfaces can be achieved through a rule-based approach, as introduced by Stephanidis et al. (1998). This methodology involves dynamically adjusting the UI to individual user needs, particularly in terms of accessibility, using predefined rules to govern how the interface should adapt under specific conditions. By leveraging contextual data and authoring tools, AUIs can provide personalized interface adjustments that cater to diverse user needs, promoting enhanced usability and inclusivity.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 10088, which is longer than the specified 7500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hallucination check passed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mariusz/Desktop/repos/wide_web_search/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, the latest trend in intelligent user interfaces is the integration of Artificial Intelligence (AI) and Augmented Reality (AR) technologies to design innovative and engaging user experiences. AI-powered tools are being used to analyze competitors' user experience, identify areas for improvement, and inform design decisions. Additionally, emerging trends such as combining AI with UX design principles and leveraging AR in UI/UX design are shaping the future of intelligent user interfaces.\n",
            "Hallucination check failed\n",
            "No relevant information item\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "import os\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "if not os.getenv(\"GOOGLE_API_KEY\") or not os.getenv(\"GOOGLE_CSE_ID\"):\n",
        "    raise EnvironmentError(\"Missing GOOGLE_API_KEY or GOOGLE_CSE_ID environment variables.\")\n",
        "\n",
        "search = GoogleSearchAPIWrapper()\n",
        "unique_urls = set()\n",
        "\n",
        "for search_query in SEARCH_QUERIES:\n",
        "    results = search.results(search_query, MAX_SOURCES_PER_SEARCH_QUERY, search_params={'dateRestrict': f'd{TIME_HORIZON}', 'gl': 'EN'})\n",
        "    urls = [item['link'] for item in results]\n",
        "    unique_urls.update(urls)\n",
        "\n",
        "source_items = {}\n",
        "for url in unique_urls:\n",
        "    loader = WebBaseLoader(url)\n",
        "    doc = loader.load()\n",
        "    title = doc[0].metadata.get('title', url)\n",
        "    source_items[title] = {'url': url, 'page_content': doc[0].page_content, 'qa':{}}\n",
        "    retriver = create_index_retriver(doc)\n",
        "    for question in CONTENT_QUESTIONS :\n",
        "        semantic_search_chunks = retriver.invoke(question)\n",
        "        relevant_chunks = []\n",
        "        for chunk in semantic_search_chunks:\n",
        "            if is_chunk_relevant(chunk, question, llm_json_mode):  \n",
        "                relevant_chunks.append(chunk)\n",
        "        if len(relevant_chunks)==0:\n",
        "            break\n",
        "        answer = generate(question, relevant_chunks, llm)\n",
        "        print(answer)\n",
        "        is_answer_make_sense = hallucination_grader(answer, relevant_chunks, llm_json_mode)\n",
        "        if is_answer_make_sense == \"yes\":\n",
        "            print(\"Hallucination check passed\")\n",
        "            source_items[title]['qa'][question] = answer \n",
        "        else:\n",
        "            print(\"Hallucination check failed\")\n",
        "            break\n",
        "    question_relevance_score = len(source_items[title]['qa'])\n",
        "    if question_relevance_score == 0:\n",
        "        source_items.pop(title)\n",
        "        print(\"No relevant information item\")\n",
        "    else:\n",
        "        summary = summarize_text_map_reduce(llm, doc, 7500)\n",
        "        source_items[title]['summary'] = summary\n",
        "        source_items[title]['question_relevance_score'] = question_relevance_score\n",
        "    \n",
        "ranked_data_items = dict(sorted(source_items.items(), key=lambda x: x[1]['question_relevance_score'], reverse=True))\n",
        "\n",
        "MAX_OUTPUTS = min(MAX_OUTPUTS, len(ranked_data_items)) \n",
        "\n",
        "top_data = {}\n",
        "less_relevant_data = {}\n",
        "for i, (title, meta_data) in enumerate(ranked_data_items.items()):\n",
        "    if i < MAX_OUTPUTS:\n",
        "        top_data[title] = meta_data\n",
        "    else:\n",
        "        less_relevant_data[title] = meta_data\n",
        "\n",
        "def remove_relevance_score(data):\n",
        "    for item in data.values():\n",
        "        if 'question_relevance_score' in item:\n",
        "            del item['question_relevance_score']\n",
        "    return data\n",
        "\n",
        "top_data = remove_relevance_score(top_data)\n",
        "less_relevant_data = remove_relevance_score(less_relevant_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ca8108c",
      "metadata": {},
      "source": [
        "# Youtube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "75d17594",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m['https://www.youtube.com/watch?v=nS1UrJnncWc&pp=ygVaSGlnaC1sZXZlbCBvdmVydmlldyBvZiBpbnRlbGxpZ2VudCB1c2VyIGludGVyZmFjZXMgYW5kIHRoZWlyIGltcGFjdCBvbiBtb2Rlcm4gVUkvVVggZGVzaWdu', 'https://www.youtube.com/watch?v=XZf5A0wcruE&pp=ygVaSGlnaC1sZXZlbCBvdmVydmlldyBvZiBpbnRlbGxpZ2VudCB1c2VyIGludGVyZmFjZXMgYW5kIHRoZWlyIGltcGFjdCBvbiBtb2Rlcm4gVUkvVVggZGVzaWdu']\u001b[0m\u001b[32;1m\u001b[1;3m['https://www.youtube.com/watch?v=MOyl58VF2ak&pp=ygVLVG9vbHMgYW5kIGZyYW1ld29ya3MgZm9yIGJ1aWxkaW5nIGludGVsbGlnZW50IHVzZXIgaW50ZXJmYWNlczogQSAyMDI0IGd1aWRl', 'https://www.youtube.com/watch?v=h2FDq3agImI&pp=ygVLVG9vbHMgYW5kIGZyYW1ld29ya3MgZm9yIGJ1aWxkaW5nIGludGVsbGlnZW50IHVzZXIgaW50ZXJmYWNlczogQSAyMDI0IGd1aWRl']\u001b[0mNo relevant information item\n",
            "No relevant information item\n",
            "No relevant information item\n",
            "No relevant information item\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader, YoutubeLoader\n",
        "import os\n",
        "from langchain_community.tools import YouTubeSearchTool\n",
        "import ast\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "if not os.getenv(\"GOOGLE_API_KEY\") or not os.getenv(\"GOOGLE_CSE_ID\"):\n",
        "    raise EnvironmentError(\"Missing GOOGLE_API_KEY or GOOGLE_CSE_ID environment variables.\")\n",
        "\n",
        "tool = YouTubeSearchTool()\n",
        "unique_urls = set()\n",
        "for search_query in SEARCH_QUERIES:\n",
        "    urls_str = tool.run(search_query, 2 * MAX_SOURCES_PER_SEARCH_QUERY)\n",
        "    urls = set(ast.literal_eval(urls_str))\n",
        "    unique_urls = unique_urls | urls\n",
        "\n",
        "unique_urls = list(unique_urls)\n",
        "\n",
        "source_items = {}\n",
        "for url in unique_urls:\n",
        "    loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)\n",
        "    doc = loader.load()\n",
        "    title = doc[0].metadata.get('title', url)\n",
        "    source_items[title] = {'url': url, 'page_content': doc[0].page_content, 'qa':{}}\n",
        "    \n",
        "    published_date_str = doc[0].metadata.get('publish_date', None)\n",
        "    published_date = datetime.strptime(published_date_str, '%Y-%m-%d %H:%M:%S')\n",
        "    if datetime.now() - published_date <= timedelta(days=TIME_HORIZON):\n",
        "        source_items.pop(title)\n",
        "        break\n",
        "    \n",
        "    retriver = create_index_retriver(doc)\n",
        "    for question in CONTENT_QUESTIONS :\n",
        "        semantic_search_chunks = retriver.invoke(question)\n",
        "        relevant_chunks = []\n",
        "        for chunk in semantic_search_chunks:\n",
        "            if is_chunk_relevant(chunk, question, llm_json_mode):  \n",
        "                relevant_chunks.append(chunk)\n",
        "        if len(relevant_chunks)==0:\n",
        "            break\n",
        "        answer = generate(question, relevant_chunks, llm)\n",
        "        print(answer)\n",
        "        is_answer_make_sense = hallucination_grader(answer, relevant_chunks, llm_json_mode)\n",
        "        if is_answer_make_sense == \"yes\":\n",
        "            print(\"Hallucination check passed\")\n",
        "            source_items[title]['qa'][question] = answer \n",
        "        else:\n",
        "            print(\"Hallucination check failed\")\n",
        "            break\n",
        "    question_relevance_score = len(source_items[title]['qa'])\n",
        "    if question_relevance_score == 0:\n",
        "        source_items.pop(title)\n",
        "        print(\"No relevant information item\")\n",
        "    else:\n",
        "        summary = summarize_text_map_reduce(llm, doc, 7500)\n",
        "        source_items[title]['summary'] = summary\n",
        "        source_items[title]['question_relevance_score'] = question_relevance_score\n",
        "    \n",
        "ranked_data_items = dict(sorted(source_items.items(), key=lambda x: x[1]['question_relevance_score'], reverse=True))\n",
        "\n",
        "MAX_OUTPUTS = min(MAX_OUTPUTS, len(ranked_data_items)) \n",
        "\n",
        "top_data = {}\n",
        "less_relevant_data = {}\n",
        "for i, (title, meta_data) in enumerate(ranked_data_items.items()):\n",
        "    if i < MAX_OUTPUTS:\n",
        "        top_data[title] = meta_data\n",
        "    else:\n",
        "        less_relevant_data[title] = meta_data\n",
        "\n",
        "def remove_relevance_score(data):\n",
        "    for item in data.values():\n",
        "        if 'question_relevance_score' in item:\n",
        "            del item['question_relevance_score']\n",
        "    return data\n",
        "\n",
        "top_data = remove_relevance_score(top_data)\n",
        "less_relevant_data = remove_relevance_score(less_relevant_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d6e27756",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_base_folder = './runs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5525cd67",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "def create_output_directory(base_dir):\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    output_dir = os.path.join(base_dir, timestamp)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    return output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9b808685",
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml    \n",
        "import os\n",
        "\n",
        "output_folder = create_output_directory(output_base_folder)\n",
        "with open(os.path.join(output_folder, \"top_data.yaml\"), \"w\") as f:\n",
        "    yaml.dump(top_data, f, default_flow_style=False, sort_keys=False)\n",
        "\n",
        "# Save rest of the sources\n",
        "with open(os.path.join(output_folder, \"less_relevant_data.yaml\"), \"w\") as f:\n",
        "    yaml.dump(less_relevant_data, f, default_flow_style=False, sort_keys=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
